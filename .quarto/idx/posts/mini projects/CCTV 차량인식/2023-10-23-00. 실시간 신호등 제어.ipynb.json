{"title":"00. 실시간 신호등 제어","markdown":{"yaml":{"title":"00. 실시간 신호등 제어","author":"GC","date":"10/20/23"},"headingText":"Intro","containsRefs":false,"markdown":"\n\n\n`-` 실시간 차량 인식 솔루션 개발\n\n* CCTV 영상 데이터를 이용하여 실시간 차량 인식\n\n* 교통상황 분석 $\\cdot$ 실시간 신호 자동제어를 통해 통행시간 감소\n\n* 일일 신호주기 세팅 + 실시간 영상 분석 = 신호주기 재설정\n\n`-` Object Detecion 기법 활용\n\n* Object Detection = `Multi-Labeled Classification` + `Bounding Box Regression`\n\n# 1. 환경 설정하기\n\n\n- 패스설정하고 Yolov5 설치\n\n## (1) 구글 드라이브 연결하기\n\n\n## (2) 데이터 불러오기\n\n## (3) 데이터셋 확인하기\n\n---\n\n## (4) 학습이미지, 테스트이미지 리스트 파일 생성하기\n- [목적] yolov5가 학습.테스트용이미지 경로가 어디있는지 알수 있게 하기 위함\n\n---\n\n## (5) 환경정보가 들어간 데이터셋인 data.yaml 데이터 설정 파일 수정\n\n\n`-` Yaml :  xml과 json 포맷과 같이 타 시스템 간에 데이터를 주고받을 때 약속된 포맷(규칙)이 정의되어있는 파일 형식으로 환경 정보가 포함되어 있다.\n\n*  [정보] ① names : class이름, ② nc : class종류수 ③ train/validation 경로\n\n* 여기서 주의 사항은 train/val 경로는 Full Path가 들어간 이미지 리스트 파일(train.txt, validation.txt)로 연결이 되어야함. 그래서, 앞서 생성된 리스트 파일이 필요\n\n* Yaml 예시\n\n```\nnames:\n- cars\n- motorbike\nnc: 2\nroboflow:\n  license: CC BY 4.0\n  project: exp2-livtl\n  url: https://universe.roboflow.com/kurs-w4uhs/exp2-livtl/dataset/2\n  version: 2\n  workspace: kurs-w4uhs\ntest: ../test/images\ntrain: EXP2-2/train/images\nval: EXP2-2/valid/images\n\n```\n\n- 이 내용과 labels의 내용을 보면, labels의 첫번째 컬럼의 값이 0이면 cars, 1이면 motorbike\n\n### a.  data.yaml 파일의 정보를 data의 변수에 딕셔너리형태로 저장\n\n ### b. `data.yaml` 파일 내 train, validation 경로 .txt 파일 경로로 변경 후 저장\n- 현재 data변수에 yaml의 자료가 Dictionary 형태로 들어가 있음\n- 값이 train,  val의 Key 값이 있으므로 여기에 변경된 경로를 추가함.\n\n# 2. YoloV5 준비\n\n### (1) YOLOv5파일 다운로드 및 설치\n\n* **인스톨페이지 참조:**\n[Install Page](https://github.com/ultralytics/yolov5)\n\n\n\n---\n\n- <font color=\"green\">**학습을 위한 데이터 세팅까지 완료되었습니다.**\n\n\n# 3. 모델 학습 및 적용\n\n## (1) Yolov5 를 이용한 모델 학습\n\n## (2) 모델 성능 확인\n\n\n> Yolo에서는 모델의 성능(정확도)를 Mean Average Precision(mAP)를 통해 확인\nmAP가 높을수록 정확하고, 작을수록 부정확\n\n> AP를 계산할 때, precision-recall, IoU 와 연관이 있음.\n\n> 정밀도 = (정확히 인식한(IOU=50%) 객체수)/시스템이 식별한 객체수\n> 재현율 = (정확히 인식한 객체수)/ 실제 객체수\n\n* 성능을 높히는 방법 참고 자료\n- https://discuss.pytorch.kr/t/python-yolov5/381\n\n* 예시\n```\nModel summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.02it/s]\n                   all         62         67      0.977      0.964      0.989      0.741\n                  cars         62         55      0.964      0.927      0.983      0.686\n             motorbike         62         12      0.991          1      0.995      0.796\n```\n\n\n\n## (3) real data 적용\n\n* 학습한 모델을 적용하여 이미지나, 영상안의 객체를 식별\n* 이미지나 영상은 미리 저장되어 있어야 함(구글드라이브에 저장)\n* 모델이 적용한 이후에 객체가 Detect된 박스가 적용된 이미지나 영상은 yolov5/runs/detect/exp/(exp2, exp3등등) 위치에 저장됨\n\n> ○ [Command]\n``` # 코드로 형식 지정됨\n!python detect.py --source 0  # webcam\n                           img.jpg  # image\n                           vid.mp4  # video\n                           screen  # screenshot\n                           path/  # directory\n                           'path/*.jpg'  # glob\n                           'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n                           'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n```\n> ○ [Properties]\n>> -- source : test 데이터(이미지, 영상 파일 혹은 폴더) 경로 <br>\n>> -- weights : 학습이 완료된 weight 파일 경로 (best.pt 형식) <br>\n>> -- conf : conf_threshold 값 (0 ~ 1 사이의 값), IOU threshold값임. 보통 0.5임\n\n\n* <font color=\"green\">**[주의] 반드시 위의 결과에서 Results saved to.... 확인 필수**\n\n## (4) Detect한 이미지 출력\n\n- **[참조] cv2 활용하지 않고 PIL(pillow)라이브러리를 활용하여 이미지 출력하기**\n\n\n\n```\nfrom PIL import Image               # to load images\nfrom IPython.display import display # to display images\n\ndetect_image_path = \"/content/yolov5/runs/detect/exp\"  #detect가 완료된 파일의 경로\n\nfor i in glob.glob(detect_image_path + '/*.jpg'):\n  img = Image.open(i)\n  img_resize = img.resize((100, 60))\n  display(img_resize)\n  print('\\n')\n```\n\n\n\n## (5) 동영상을 소스로 한 객체 검출\n\n*  ① IOU Threshold : 0.1, ② 모델 weights : best.pt ③ 이미지사이즈 640\n\n## (6) 동영상 확인\n","srcMarkdownNoYaml":"\n\n# Intro\n\n`-` 실시간 차량 인식 솔루션 개발\n\n* CCTV 영상 데이터를 이용하여 실시간 차량 인식\n\n* 교통상황 분석 $\\cdot$ 실시간 신호 자동제어를 통해 통행시간 감소\n\n* 일일 신호주기 세팅 + 실시간 영상 분석 = 신호주기 재설정\n\n`-` Object Detecion 기법 활용\n\n* Object Detection = `Multi-Labeled Classification` + `Bounding Box Regression`\n\n# 1. 환경 설정하기\n\n\n- 패스설정하고 Yolov5 설치\n\n## (1) 구글 드라이브 연결하기\n\n\n## (2) 데이터 불러오기\n\n## (3) 데이터셋 확인하기\n\n---\n\n## (4) 학습이미지, 테스트이미지 리스트 파일 생성하기\n- [목적] yolov5가 학습.테스트용이미지 경로가 어디있는지 알수 있게 하기 위함\n\n---\n\n## (5) 환경정보가 들어간 데이터셋인 data.yaml 데이터 설정 파일 수정\n\n\n`-` Yaml :  xml과 json 포맷과 같이 타 시스템 간에 데이터를 주고받을 때 약속된 포맷(규칙)이 정의되어있는 파일 형식으로 환경 정보가 포함되어 있다.\n\n*  [정보] ① names : class이름, ② nc : class종류수 ③ train/validation 경로\n\n* 여기서 주의 사항은 train/val 경로는 Full Path가 들어간 이미지 리스트 파일(train.txt, validation.txt)로 연결이 되어야함. 그래서, 앞서 생성된 리스트 파일이 필요\n\n* Yaml 예시\n\n```\nnames:\n- cars\n- motorbike\nnc: 2\nroboflow:\n  license: CC BY 4.0\n  project: exp2-livtl\n  url: https://universe.roboflow.com/kurs-w4uhs/exp2-livtl/dataset/2\n  version: 2\n  workspace: kurs-w4uhs\ntest: ../test/images\ntrain: EXP2-2/train/images\nval: EXP2-2/valid/images\n\n```\n\n- 이 내용과 labels의 내용을 보면, labels의 첫번째 컬럼의 값이 0이면 cars, 1이면 motorbike\n\n### a.  data.yaml 파일의 정보를 data의 변수에 딕셔너리형태로 저장\n\n ### b. `data.yaml` 파일 내 train, validation 경로 .txt 파일 경로로 변경 후 저장\n- 현재 data변수에 yaml의 자료가 Dictionary 형태로 들어가 있음\n- 값이 train,  val의 Key 값이 있으므로 여기에 변경된 경로를 추가함.\n\n# 2. YoloV5 준비\n\n### (1) YOLOv5파일 다운로드 및 설치\n\n* **인스톨페이지 참조:**\n[Install Page](https://github.com/ultralytics/yolov5)\n\n\n\n---\n\n- <font color=\"green\">**학습을 위한 데이터 세팅까지 완료되었습니다.**\n\n\n# 3. 모델 학습 및 적용\n\n## (1) Yolov5 를 이용한 모델 학습\n\n## (2) 모델 성능 확인\n\n\n> Yolo에서는 모델의 성능(정확도)를 Mean Average Precision(mAP)를 통해 확인\nmAP가 높을수록 정확하고, 작을수록 부정확\n\n> AP를 계산할 때, precision-recall, IoU 와 연관이 있음.\n\n> 정밀도 = (정확히 인식한(IOU=50%) 객체수)/시스템이 식별한 객체수\n> 재현율 = (정확히 인식한 객체수)/ 실제 객체수\n\n* 성능을 높히는 방법 참고 자료\n- https://discuss.pytorch.kr/t/python-yolov5/381\n\n* 예시\n```\nModel summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.02it/s]\n                   all         62         67      0.977      0.964      0.989      0.741\n                  cars         62         55      0.964      0.927      0.983      0.686\n             motorbike         62         12      0.991          1      0.995      0.796\n```\n\n\n\n## (3) real data 적용\n\n* 학습한 모델을 적용하여 이미지나, 영상안의 객체를 식별\n* 이미지나 영상은 미리 저장되어 있어야 함(구글드라이브에 저장)\n* 모델이 적용한 이후에 객체가 Detect된 박스가 적용된 이미지나 영상은 yolov5/runs/detect/exp/(exp2, exp3등등) 위치에 저장됨\n\n> ○ [Command]\n``` # 코드로 형식 지정됨\n!python detect.py --source 0  # webcam\n                           img.jpg  # image\n                           vid.mp4  # video\n                           screen  # screenshot\n                           path/  # directory\n                           'path/*.jpg'  # glob\n                           'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n                           'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n```\n> ○ [Properties]\n>> -- source : test 데이터(이미지, 영상 파일 혹은 폴더) 경로 <br>\n>> -- weights : 학습이 완료된 weight 파일 경로 (best.pt 형식) <br>\n>> -- conf : conf_threshold 값 (0 ~ 1 사이의 값), IOU threshold값임. 보통 0.5임\n\n\n* <font color=\"green\">**[주의] 반드시 위의 결과에서 Results saved to.... 확인 필수**\n\n## (4) Detect한 이미지 출력\n\n- **[참조] cv2 활용하지 않고 PIL(pillow)라이브러리를 활용하여 이미지 출력하기**\n\n\n\n```\nfrom PIL import Image               # to load images\nfrom IPython.display import display # to display images\n\ndetect_image_path = \"/content/yolov5/runs/detect/exp\"  #detect가 완료된 파일의 경로\n\nfor i in glob.glob(detect_image_path + '/*.jpg'):\n  img = Image.open(i)\n  img_resize = img.resize((100, 60))\n  display(img_resize)\n  print('\\n')\n```\n\n\n\n## (5) 동영상을 소스로 한 객체 검출\n\n*  ① IOU Threshold : 0.1, ② 모델 weights : best.pt ③ 이미지사이즈 640\n\n## (6) 동영상 확인\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"output-file":"2023-10-23-00. 실시간 신호등 제어.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","theme":{"light":"cosmos"},"editor":"visual","code-copy":true,"title-block-banner":true,"title":"00. 실시간 신호등 제어","author":"GC","date":"10/20/23"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}